{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Neural network"
      ],
      "metadata": {
        "id": "A0sKE3dyJ5dt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMP6GMxEJ0Lh"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data # attributes\n",
        "y = iris.target # labels\n",
        "\n",
        "# Convert to a DataFrame for easier exploration\n",
        "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "iris_df[\"target\"] = iris.target\n",
        "\n",
        "# Display the first few rows\n",
        "iris_df.head()\n",
        "\n",
        "# One-hot encode the target variable\n",
        "print(\"One-hot encoding\")\n",
        "#print(y)\n",
        "y = to_categorical(y, num_classes=3)\n",
        "#print(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the feature data\n",
        "# The goal of standardization is to transform the feature data so that it has a mean of zero and a standard\n",
        "# deviation of one, ensuring that each feature contributes equally to the model.\n",
        "# fit_transform computes the mean and standard deviation of the features and apply the transformation.\n",
        "# This ensures that the training data is standardized based on its own statistics.\n",
        "# the test data is standardized using the same mean and standard deviation values calculated from the training data.\n",
        "# By applying transform to X_test the test data is standardized in the same way, without recalculating the mean and standard deviation for the test set.\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model (constructor)\n",
        "# Hidden layer with 10 neurons receives input from the training data (X_train)\n",
        "# The input dimension is determined by the number of features in the data (X_train.shape[1]).\n",
        "# ReLU introduces non-linearity into the model. It activates the neurons by outputting the input directly if it is positive,\n",
        "# and outputs zero if the input is negative, thereby allowing the model to learn non-linear patterns.\n",
        "# The output layer has 3 neurons (the three possible classes in the Iris dataset). The activation function softmax\n",
        "# (multi-class classification problem) outputs a probability distribution across the three classes, where the sum of the probabilities for\n",
        "# all classes is 1. The class with the highest probability is the predicted class for a given input.\n",
        "# Each layer plays a vital role in transforming the input data into meaningful output for classification, with\n",
        "# the hidden layer learning the features and the output layer providing the final classification.\n",
        "model = Sequential([\n",
        "    Dense(10, input_dim=X_train.shape[1], activation=\"relu\"), # first param is hidden layer and second the input layer\n",
        "    Dense(3, activation=\"softmax\") # Output layer with 3 neurons (one for each class)\n",
        "])\n",
        "\n",
        "\n",
        "# Build the neural network model using the .add() syntax (setter)\n",
        "# model = Sequential()\n",
        "# model.add(Dense(10, input_dim=X_train.shape[1], activation=\"relu\")) # Hidden layer with 10 neurons\n",
        "# model.add(Dense(3, activation=\"softmax\")) # Output layer with 3 neurons (one for each class)\n",
        "\n",
        "# Compile the model\n",
        "# Compiling the model configures the learning process by specifying the optimization algorithm, the loss function, and the evaluation metrics.\n",
        "# In this case, we use the compile method to define these elements.\n",
        "# The optimizer is responsible for updating the model’s weights during training. We use the Adam (one of the most common in deep learning).\n",
        "# It combines Adagrad and RMSProp algorithms, and adapts the learning rate for each parameter. It is well-suited for problems with large datasets\n",
        "# and high-dimensional parameter spaces.\n",
        "# The loss function measures how well the model’s predictions match the true labels. We use categorical_crossentropy and it is used when there are\n",
        "# multiple, mutually exclusive classes. It calculates the difference between the predicted probability distribution and the actual class label,\n",
        "# which is one-hot encoded.\n",
        "# The metrics determine how we evaluate the model’s performance during training and testing. The 'accuracy' measures the proportion of correct\n",
        "# predictions out of all predictions.\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "# The epochs parameter defines how many times the model will iterate over the entire training dataset.\n",
        "# A high number of epochs may lead to overfitting if the model is trained for too many epochs without proper regularization.\n",
        "# The batch_size parameter specifies how many samples the model will process before updating its weights.\n",
        "# The validation_split parameter is set to 0.2, meaning that 20% of the training data will be used for validation during training.\n",
        "# The training process will return a history object, which contains the loss and accuracy values for both the training\n",
        "# and validation sets at each epoch.\n",
        "print(\"HISTORY\")\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=8, validation_split=0.2)\n",
        "\n",
        "# Evaluate model\n",
        "# Accuracy: percentage of correct predictions made by the model on the test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.2f}\\nTest Loss: {loss:.2f}\")\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "print(f\"\\nPredictions\\n{predictions}\")\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "# A confusion matrix is a performance measurement tool for classification problems, particularly in supervised learning.\n",
        "# It is a table that describes the performance of a classification model by comparing the predicted labels with the true labels.\n",
        "# The matrix allows us to see not only the errors made by the classifier but also the types of errors.\n",
        "# Each row of the matrix represents the true class, while each column represents the predicted class.\n",
        "print(\"\\nConfusion matrix\")\n",
        "conf_matrix = confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1))\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\",\"Class 1\", \"Class 2\"], yticklabels=[\"Class 0\", \"Class 1\", \"Class 2\"])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "plt.ylabel(\"True Class\")\n",
        "plt.show()\n",
        "\n",
        "# The first row of the confusion matrix indicates that there are 10 instances of Class 0 (setosa) correctly classified, and none misclassified as\n",
        "# other classes.\n",
        "# The second row shows that 8 instances of Class 1 (versicolor) were correctly predicted, and 1 instance was misclassified as Class 2 (virginica).\n",
        "# The third row shows that 11 instances of Class 2 (virginica) were correctly predicted, with no misclassifications.\n",
        "# We aim for all numbers to be on the main diagonal, meaning that the model has correctly classified every instance without any misclassifications.\n",
        "\n",
        "print(\"\\nPlotting the predicted classes distribution\")\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.hist(predictions.argmax(axis=1), bins=3, rwidth=0.8, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Distribution of Predicted Classes\")\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "plt.ylabel(\"Number of Instances\")\n",
        "plt.xticks([0, 1, 2], ['Class 0', 'Class 1', 'Class 2'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ONE-HOT ENCODING\n",
        "One-hot encoding is a method used to convert categorical variables into a numerical format that can be easily interpreted by machine learning algorithms. In many real-world datasets, the features or target variables are categorical, meaning they represent distinct categories or classes, such as colors, types of animals, or different product categories. However, most machine learning algorithms require numerical input to perform calculations. One-hot encoding solves this problem by converting each category into a binary vector where only one element is \"hot\" (set to 1) and the others are \"cold\" (set to 0). For example, consider a simple dataset with a categorical variable \"color\" which can have three possible values: red, green, and blue. Using one-hot encoding, each of these categories would be represented as a binary vector. The color \"red\" might be encoded as [1, 0, 0], \"green\" as [0, 1, 0], and \"blue\" as [0, 0, 1]. This binary representation allows algorithms to understand the categorical nature of the data without assuming any inherent order or relationship between the categories, which could be the case if we were to simply assign numeric values like 1, 2, and 3 to represent the colors. One-hot encoding is particularly important because it ensures that machine learning models do not mistakenly interpret categorical data as ordinal, meaning they will not assume any hierarchy or ranking between categories."
      ],
      "metadata": {
        "id": "Yi4jGKkTLH0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 6"
      ],
      "metadata": {
        "id": "YvDHE2XyMcU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the CSV data and\n",
        "print(\"\\nWINES\")\n",
        "wines = '/content/drive/MyDrive/Colab Notebooks/wines.csv'\n",
        "data = pd.read_csv(wines)\n",
        "X = data.iloc[:, :-2].values  # Features or attributes include columns except the last two (quality and label)\n",
        "y = data['quality'].values    # The 'quality' column as the target label\n",
        "feature_names = data.columns[:-2].tolist() # Get the feature names\n",
        "\n",
        "# Convert to a DataFrame for easier exploration\n",
        "wines_df = pd.DataFrame(data.iloc[:, :-2].values, columns=data.columns[:-2])  # Feature columns\n",
        "wines_df[\"target\"] = data['quality']  # Add the target column\n",
        "\n",
        "# print(\"Check if noise in data set\")\n",
        "# print(wines_df[wines_df.isnull().any(axis=1)])\n",
        "# print(\"Number of unique classes and their values\")\n",
        "unique_classes = data['quality'].nunique()\n",
        "# print(data['quality'].dropna().unique())\n",
        "\n",
        "# One-hot encoding\n",
        "y = to_categorical(y - y.min(), num_classes=unique_classes)\n",
        "# print(y[:10])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the feature data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model (constructor)\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=X_train.shape[1], activation=\"relu\"), # first param is hidden layer and second the input layer\n",
        "    Dense(unique_classes, activation=\"softmax\") # Output layer with 3 neurons (one for each class)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "print(\"HISTORY\")\n",
        "history = model.fit(X_train, y_train, epochs=25, batch_size=150, validation_split=0.2)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.2f}\\nTest Loss: {loss:.2f}\")\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "print(f\"\\nPredictions\\n{predictions}\")\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "print(\"\\nConfusion matrix\")\n",
        "conf_matrix = confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1))\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(\n",
        "    conf_matrix,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=[\"Quality 3\", \"Quality 4\", \"Quality 5\", \"Quality 6\", \"Quality 7\", \"Quality 8\"],\n",
        "    yticklabels=[\"Quality 3\", \"Quality 4\", \"Quality 5\", \"Quality 6\", \"Quality 7\", \"Quality 8\"]\n",
        "    )\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "plt.ylabel(\"True Class\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPlotting the predicted classes distribution\")\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.hist(predictions.argmax(axis=1), bins=7, rwidth=0.8, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Distribution of Predicted Classes\")\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "plt.ylabel(\"Number of Instances\")\n",
        "plt.xticks([0, 1, 2, 3, 4, 5, 6], ['Class 0', 'Class 1', 'Class 2', \"Class 3\", \"Class 4\", \"Class 5\", \"Class 6\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iBahfR1HOx5K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}