{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "kNN Classification"
      ],
      "metadata": {
        "id": "11eDGMX6G2D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "# In ML features are marked with capital 'X' and labels with lowercase 'y'\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "print(\"Label names (categories or classes):\", target_names)\n",
        "print(\"Feature names (vector components):\", feature_names)\n",
        "print(\"\\nFirst 5 rows of Data (X):\\n\", X[:5])\n",
        "\n",
        "# Split the dataset into two parts: the training set and the test set i.e. train_test_split()\n",
        "# The 'test_size = 0.3' means that 30% of the data should be allocated to the test data set and\n",
        "# 70% for the training\n",
        "# The 'random_state=1' ensures the same split of data every time the code runs, making results\n",
        "# consistent and reproducible. Without it, the data is split differently with each code run.\n",
        "# In k-NN, there is no explicit training process or model parameters to optimize,\n",
        "# so a validation set is not strictly necessary.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
        "print(\"\\nSHAPES\")\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "# Invoke kNN Algorithm that is already integrated in Sklearn i.e. KNeighborsClassifier(n_neighbors = 3)\n",
        "# The model is stored in 'classifier_knn' variable\n",
        "# The 'fit' function trains the model i.e. it applies the kNN to the test data set to determine its performance\n",
        "# The 'predict' function tests the model with new (test) data\n",
        "# The model is stored in 'y_pred' variable\n",
        "# In the end we measure the accuracy\n",
        "classifier_knn = KNeighborsClassifier(n_neighbors = 3) # 3 closest neighbors\n",
        "classifier_knn.fit(X_train, y_train)\n",
        "y_pred = classifier_knn.predict(X_test)\n",
        "print(\"\\nAccuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Uncomment block of code below to see the accuracies for knn 1 - 105\n",
        "# accuracies = {}\n",
        "# for i in range(1, 106):\n",
        "#   classifier_knn = KNeighborsClassifier(n_neighbors = int(i)) # 3 closest neighbors\n",
        "#   classifier_knn.fit(X_train, y_train)\n",
        "#   y_pred = classifier_knn.predict(X_test)\n",
        "#   accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "#   accuracies = {i: accuracy}\n",
        "#   print(accuracies)\n",
        "\n",
        "\n",
        "\n",
        "# QUESTIONS\n",
        "# 1) Which function is used to split the dataset?\n",
        "# The train_test_split() splits the dataset into the training and test data sets\n",
        "# The 'test_size = 0.3' means that 30% of the data is allocated to the test and 70% to the training data\n",
        "# The 'random_state=1' ensures the same split of data every time the code runs, making results\n",
        "# consistent and reproducible. Without it, the data would be split differently with each code run.\n",
        "\n",
        "# 2) What parameters does this function (train_test_split) accept (check the function’s documentation)?\n",
        "# According to the documentation, the parameters are:\n",
        "# - X: features\n",
        "# - y: target variables (categories or classes)\n",
        "# - test_size: proportion of the test data based on the provided data set\n",
        "# - train_size: proportion of the training data based on the provided data set\n",
        "# - random_state: ensures the same split of data every time the code runs, making results consistent and reproducible. Without it, the data is split differently with each code run\n",
        "# - shuffle: whether to shuffle the data before splitting. If set to False, the data will be split in the original order\n",
        "# - stratify: ensures the split maintains the same proportion of each class as in the original dataset, which is especially useful for imbalanced datasets\n",
        "\n",
        "# 3) What are the shapes of the resulting datasets?\n",
        "# X_train (training features): (105, 4) — 105 samples, each with 4 features.\n",
        "# X_test (testing features): (45, 4) — 45 samples, each with 4 features\n",
        "\n",
        "# 4) Is this a large dataset?\n",
        "# The Iris dataset is not a large data set. It contains only 150 samples (105 for training, 45 for testing).\n",
        "\n",
        "# 4) What is a large dataset?\n",
        "# A large dataset would probably contain thousands of samples, depending on the problem at hand.\n",
        "# A dataset is considered large when it:\n",
        "# - requires significant memory or storage\n",
        "# - contains many samples, for example 1,000 or more for images or millions or text data\n",
        "# - demands distributed computing or specialized tools to process efficiently\n",
        "\n",
        "# 5) Why don’t we have a validation split here?\n",
        "# In k-NN, there is no explicit training process or model parameters to optimize,\n",
        "# so a validation set is not strictly necessary.\n",
        "# In this example, the dataset is small, and cross-validation is typically used to evaluate\n",
        "# the model instead of a separate validation split. The cross-validation would mean split the data set into\n",
        "# pairs of trainig-test data sets and multiple trainigs and testings. For simplicity, the code focuses on\n",
        "# splitting into one training and one testing set and it then evaluates accuracy on the test set.\n",
        "\n",
        "\n",
        "# 6) How is k-NN imported and called (invoked)?\n",
        "# The import is done in the following way: from sklearn.neighbors import KNeighborsClassifier\n",
        "# In order to call it, we instance the KNeighborsClassifier by specifying the number of\n",
        "# neighbors (n_neighbors).\n",
        "\n",
        "\n",
        "# 7) What is the value of k?\n",
        "# The k in our case is 3. Therefore, the algorithm will consider the 3 closest data points\n",
        "# to classify our new samples.\n",
        "\n",
        "# 8) How can value of k determine algorithm’s accuracy?\n",
        "# Just as we learned in our lectures, small k leads to overfitting, while the large k leads to\n",
        "# underfitting. In case of small k e.g. 1, the model wil make decisions based on a single nearest neighbor.\n",
        "# On the other hand, in case of large k would average over many points and misrepresent the structure.\n",
        "# The best-case scenario would be to choose the right k that balances bias and variance to provide\n",
        "# the best accuracy. A common practice could include cross-validation or a grid search to test different\n",
        "# k values and find the one that maximizes performance on validation data.\n",
        "\n",
        "# 9) Which function is used to train the classifier model?\n",
        "# The 'fit' method takes the training data and their labels, 'trains' the k-NN model by\n",
        "# storing the data and preparing it for predictions based on the k-NN algorithm.\n",
        "# According to the documentation, the k-NN is a lazy learning algorithm, which means that it does not\n",
        "# perform actual 'training' in the traditional sense. Instead, it simply stores the training data for\n",
        "# use during prediction.\n",
        "\n",
        "# 10) What is even done in the training phase of this algorithm?\n",
        "# According to the documentation, the k-NN is a lazy learning algorithm, which means that it does not\n",
        "# perform actual 'training' in the traditional sense. Instead, it stores the training data for\n",
        "# use during prediction. In a nutshell, the algorithm calculates distances between the new data point\n",
        "# and all stored training points to find the k-nearest neighbors.\n",
        "\n",
        "# 11) What is the accuracy of this algorithm?\n",
        "# The accuracy of the algorithm is 0.9777777777777777, or approximately 97.78%, which means that the\n",
        "# model correctly predicted the class of 97.78% of the test samples, and that it misclassified 2.22%\n",
        "# of the test samples. Accuracy = Number of Correct Predictions / Total Number of Predictions\n",
        "\n",
        "# 12) Is this accuracy satisfactory?\n",
        "# For our dataset and purposes, 97.78% is a satisfactory accuracy.\n",
        "\n",
        "# 13) Is this algorithm applicable in the real world situations?\n",
        "#  Real-world applications would probably require additional evaluation metrics and testing on larger\n",
        "# and more complex datasets, so that true performance can be determined with a bit more reliability.\n",
        "\n",
        "# 14) Repeat this process for several different values of k. Does the result change?\n",
        "# I was interested in the dynamics of predictions, so I wrote a loop to test the accuracy based on k value that\n",
        "# ranges from 1 to 105. Although the result changes based on the k value, most of the time it does not change with every\n",
        "# change of the value of k. Rather, there are ranges in which change of the value of k does not change\n",
        "# the accuracy. See question 8 above for a bit more details.\n",
        "# k = 1 - 31:   0.9777777777777777\n",
        "# k = 32 - 44:  0.9555555555555556\n",
        "# k = 45 - 48:  0.9333333333333333\n",
        "# k = 49 - 58:  0.9111111111111111\n",
        "# k = 59 - 60:  0.8222222222222222\n",
        "# k = 61 - 64:  0.8444444444444444\n",
        "# k = 65 - 66:  0.7333333333333333\n",
        "# k = 67 - 69:  0.7111111111111111\n",
        "# k = 70 - 71:  0.6888888888888889\n",
        "# k = 72 - 86:  0.6666666666666666\n",
        "# k = 87 - 94:  0.6444444444444445\n",
        "# k = 95:       0.6222222222222222\n",
        "# k = 96 - 104: 0.6\n",
        "# k = 105:      0.28888888888888886\n",
        "\n",
        "# 15) If so, are the changes drastic, and what is the ’optimal’ value of k?\n",
        "# In the previous question, it is explained how the value of k affects the accuracy. The best accuracy\n",
        "# is achieved with the k between 1 and 31, which tells us that the sample is pretty small. The changes\n",
        "# between the small and large values of k are drastic.\n",
        "\n",
        "# 16) What would happen if we set the value of k equal to the number of samples in the test dataset?\n",
        "# That would lead to underfitting and very low accuracy. See question 8 and 14 above.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wMa5fBL5dpHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple kNN algorithm"
      ],
      "metadata": {
        "id": "arlrsRv1hFAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Calculate the vector distance\n",
        "# distances = [0.4582575694955844, 0.8124038404635958, 3.263433774416144, 0.3316624790355399, 0.648074069840786, 3.4102785809959864]\n",
        "def euclidean_distance(test_row, train_row):\n",
        "    return np.sqrt(np.sum((test_row - train_row) ** 2))\n",
        "\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "\n",
        "    # No real training in kNN, just data initialization\n",
        "    # In k-NN, there is no explicit training process or model parameters to optimize, so a validation set is not strictly necessary.\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "\n",
        "    # determine k nearest neighbors (rows) and take classes from these rows\n",
        "    def predict(self, X_test):\n",
        "        prediction = []\n",
        "\n",
        "        for test_row in X_test:\n",
        "            distances = [euclidean_distance(test_row, train_row) for train_row in self.X_train]\n",
        "            k_distance_indices = np.argsort(distances)[:self.k]\n",
        "            k_nearest_classes = [self.y_train[index] for index in k_distance_indices]\n",
        "            prediction.append(Counter(k_nearest_classes).most_common(1))\n",
        "        return np.array(prediction)\n",
        "\n",
        "\n",
        "\n",
        "# TEST THE CODE\n",
        "# Any number of rows can be added to the data set below\n",
        "# The loop ensures that the number of classes in array matches the number of rows\n",
        "classes = []\n",
        "data = []\n",
        "X_test = np.array([[5.5, 3.5, 1.6, 0.3], [6.5, 3.7, 4.6, 1.3]])\n",
        "\n",
        "data_with_labels = np.array([[5.1, 3.5, 1.4, 0.2, 'setosa'],\n",
        "                             [4.9, 3.0, 1.4, 0.2, 'versicolor'],\n",
        "                             [6.7, 3.1, 4.4, 1.4, 'virginica'],\n",
        "                             [5.2, 3.6, 1.5, 0.3, 'setosa'],\n",
        "                             [5.0, 3.1, 1.5, 0.3, 'versicolor'],\n",
        "                             [6.8, 3.2, 4.5, 1.5, 'virginica']])\n",
        "\n",
        "for row in data_with_labels:\n",
        "    classes.append(row[4])\n",
        "    data.append(row[:4])\n",
        "\n",
        "\n",
        "y_train = np.array(classes)\n",
        "X_train = np.array(data, dtype=float)\n",
        "model = KNN(k=3)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "for i, row in enumerate(y_pred):\n",
        "  print(\"\\nThe sample\", X_test[i], \"is of the class\", row[0][0])\n",
        "  print(\"The sample\", X_test[i], \"matched\", row[0][1], \"data sets of this class.\")\n"
      ],
      "metadata": {
        "id": "7tJ5HddahJI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression"
      ],
      "metadata": {
        "id": "DHiFmIC_8U1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import sys\n",
        "#sys.exit(\"Stopping the script execution.\")\n",
        "\n",
        "\n",
        "diabetes = load_diabetes()\n",
        "X = diabetes.data\n",
        "y = diabetes.target\n",
        "# print(\"Features: \", diabetes.feature_names)\n",
        "# print(\"Target: \", diabetes.target)\n",
        "\n",
        "# Only the third feature (index 2) required\n",
        "X_feature = X[:, 2].reshape(-1, 1) # convert the 3rd feature from 1D into a 2D array (one column)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_feature, y, test_size=0.15, random_state=1)\n",
        "\n",
        "\n",
        "# Initialize the linear regression model, train it and predict\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Coefficients of the regression model and Mean Squared Error of the predictions\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Coefficients: \", model.coef_)\n",
        "print(\"Mean Squared Error: \", mse)\n",
        "\n",
        "\n",
        "# Scatter plot of the actual values from the testing set\n",
        "# Overlay the linear regression line corresponding to the predicted values\n",
        "plt.scatter(X_test, y_test, color='blue', label='Actual values')\n",
        "plt.plot(X_test, y_pred, color='red', label='Linear regression line')\n",
        "plt.xlabel('Feature: BMI')\n",
        "plt.ylabel('Disease Progression')\n",
        "plt.title('Linear Regression on Diabetes Dataset (Feature: BMI)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1) What are the features, and what are the targets?\n",
        "# Features: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
        "# Targets: [151.  75. 141. 206. 135.  97. 138.  63. 110. 310......]\n",
        "\n",
        "# 2) At the end, see what would happen if the first or second feature are used?\n",
        "# The first feature is 'age' which means we have a model that predicts the disease progression based on the patients' age.\n",
        "# Second feature is 'sex' which means we have a model that predicts disease based on patients' gender."
      ],
      "metadata": {
        "id": "Qth9Uqc48YbF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}