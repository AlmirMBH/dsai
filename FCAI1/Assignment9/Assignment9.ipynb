{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Natural Language Processing (NLP)\n",
        "Natural Language Processing (NLP) is a subfield of Artificial Intelligence (AI) that aims to bridge the gap between human communication and computational systems by enabling machines to interpret, understand, and generate human language. NLP combines linguistics (the structure and meaning of language) with machine learning algorithms to process and analyze large amounts of natural language data.\n",
        "\n",
        "- Text preprocessing - Implies text cleaning by removing unnecessary characters or sequences (e.g. HTML tags, punctuation, or any other non-alphanumeric characters), casing transformation, stopwords removal etc.\n",
        "Stopwords are any words that do not carry significant meaning or contribute much to the analysis in most cases. These words are\n",
        "usually high-frequency, functional words such as articles, conjunctions, prepositions, and pronouns, which help structure sentences but\n",
        "don’t provide useful information.\n",
        "- Tokenization - Process of splitting text into smaller units called tokens. These tokens can be words, subwords, or characters, depending on the tokenization method used, and serve as the basic units for further text analysis or processing.\n",
        "- Vectorization - Transforming tokens (e.g. words) into numerical representation (usually n-dimensional vectors).\n",
        "\n",
        "The most common NLP tasks are:\n",
        "- Language Understanding - For example identifying entities like names, dates, locations etc. (NER - Named Entity Recognition), labeling words as nouns, verbs, adjectives, etc. (Part-of-Speech Tagging) or analyzing the\n",
        "grammatical structure of sentences (Syntactic Parsing).\n",
        "- Sentiment Analysis - Determining the emotional tone or sentiment expressed in a piece of text, like whether a review is positive or negative.\n",
        "- Machine Translation - Automatically translating text from one language to another (e.g., Google Translate).\n",
        "- Text Generation - Creating new text based on learned patterns (e.g.,generating coherent responses in chatbots or summarizing long documents).\n",
        "- Speech Recognition - Converting spoken language into written text (used in virtual assistants like Siri or Alexa)."
      ],
      "metadata": {
        "id": "ob3bGtDM4ngq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QanBaawg4JAQ",
        "outputId": "40a5db9e-aa17-44ae-a6fa-4fcbcdafbdc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "\u001b[1m84125825/84125825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from keras.utils import get_file\n",
        "\n",
        "\n",
        "# Download dataset from provided URL\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "# url = '/content/drive/MyDrive/Colab Notebooks/aclImdb'\n",
        "dataset = get_file(\"aclImdb_v1\", url, untar=True, cache_dir=\".\", cache_subdir=\"\")\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data"
      ],
      "metadata": {
        "id": "ltXyUEcG-9OH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from keras.utils import text_dataset_from_directory\n",
        "\n",
        "\n",
        "# Define train path\n",
        "dataset_dir = '/content/drive/MyDrive/Colab Notebooks/aclImdb'\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "\n",
        "# Remove additional unsup/ directory from the train/ directory\n",
        "remove_dir = os.path.join(train_dir, \"unsup\")\n",
        "if os.path.exists(remove_dir):\n",
        "  shutil.rmtree(remove_dir)"
      ],
      "metadata": {
        "id": "1pLeXW3W8NjG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model\n",
        "The text_dataset_from_directory function scans the train/ directory for subdirectories (i.e., pos/ and neg/) and uses their names as class labels. It reads each text file in the subdirectories and assigns the text content to a dataset sample and the corresponding class label based on the subdirectory."
      ],
      "metadata": {
        "id": "oe2fIRQD_DhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import text_dataset_from_directory\n",
        "\n",
        "train_dataset = '/content/drive/MyDrive/Colab Notebooks/aclImdb/train'\n",
        "# Load train dataset\n",
        "train_dataset = text_dataset_from_directory(train_dataset, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI9GFv5F_Udo",
        "outputId": "64976e18-2b0a-4551-bbcd-0ee72d8e6739"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25020 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text pre-processing\n",
        "Individual reviews are stored in separate text files. This function takes one input argument called text which represents a single review read from the text file. First, function transforms all letters to lowercase, then using a simple regex removes all HTML tags and non-alphanumerical characters from the string, and finally returns cleaned text. This is a custom defined function and could be further expanded or redefined by using additional cleaning techniques."
      ],
      "metadata": {
        "id": "TU-ZFAg1_xt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.strings import lower, regex_replace\n",
        "\n",
        "def preprocess(text):\n",
        "  # Lower casing\n",
        "  text = lower(text)\n",
        "\n",
        "  # Remove HTML tags\n",
        "  text = regex_replace(text, \"<br />\", \" \")\n",
        "\n",
        "  # Remove special characters and punctuation\n",
        "  text = regex_replace(text, \"[^A-Za-z0-9]+\", \" \")\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "kLgtpIAi_2zs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TextVectorization Layer\n",
        "The TextVectorization layer in Keras is a powerful preprocessing layer that transforms raw text into numeric\n",
        "tensors, preparing the text for ML/AI models. It involves three main steps: standardization (i.e., text preprocessing), tokenization, and vectorization. Each of these steps is configurable, making this layer highly adaptable to different text preprocessing needs.\n",
        "\n",
        "In the code snippet above, we first initialize a TextVectorization layer by defining its key parameters: preprocessing function for standardization, maximum vocabulary size, output mode and output sequence length. After the layer has been created, the adapt is called on the list of strings from the train dataset in order to create the vocabulary.\n",
        "Detailed breakdown of each step in the TextVectorization pipeline is provided below:\n",
        "1. Standardize each example - This step is the first part of the TextVectorization process. Its purpose is to clean and normalize the input text to ensure uniformity, regardless of variations like casing, punctuation,\n",
        "or other noise in the data. Although Keras provides a default standardization processes that can be applied\n",
        "automatically (e.g. lower_and_strip_punctuation), in your case, a custom standardization function (preprocess) is used, overriding this default behavior.\n",
        "2. Split each example into substrings - Split the standardized text into substrings, typically words. By\n",
        "default, the TextVectorization layer splits the example by whitespace (which means that each substring is an individual word).\n",
        "3. Recombine substrings into tokens - Tokens are the building blocks of text that have been extracted from\n",
        "the raw text data. In the simplest case (like ours), each resulting substring (i.e., word) is treated as a token.\n",
        "However, tokens do not have to be single words. In fact, tokens can also be a combination of words or characters\n",
        "(this can be accomplished by setting the ngram parameter).\n",
        "4. Index tokens - In this step each unique token is associated with an integer index based on the vocabulary\n",
        "built during the adapt step. When TextVectorization layer is adapted, it will analyze the train dataset, determine the frequency of individual string values, and create a vocabulary from them. This vocabulary can have unlimited size or can be capped. In our case, the vocabulary is capped to 10,000 words. This means that the least frequent words are removed so that only the 10,000 most frequent ones are used to create the vocabulary.\n",
        "5. Transform each example using the index - The goal of this step is to transform the array of token indices into a fixed-length output. In our case, the output is an array of exactly 250 integers. If the array is shorter than 250,array is padded (i.e., zeros are added), and if it is longer, it is truncated to exactly 250 elements."
      ],
      "metadata": {
        "id": "zL9mLHC4A3PD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import TextVectorization\n",
        "\n",
        "\n",
        "# Initialize TextVectorization layer\n",
        "vectorize_layer = TextVectorization(\n",
        "  standardize=preprocess, # Custom standardization function\n",
        "  max_tokens=10000, # Maximum size of the vocabulary\n",
        "  output_mode=\"int\", # Output of this layer will be a sequence of integer indices\n",
        "  output_sequence_length=250 # Pad or truncate sequences to exactly 250 values\n",
        ")\n",
        "\n",
        "# Extract text from train dataset (without labels)\n",
        "trainX = [x for x, _ in train_dataset.unbatch()]\n",
        "\n",
        "# Call adapt on the list of strings to create the vocabulary\n",
        "vectorize_layer.adapt(trainX)"
      ],
      "metadata": {
        "id": "6GmvfTclCnSJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying the TextVectorization Layer\n",
        "The TextVectorization layer in Keras expects each input sample to be a 1D tensor, which represents a single string. This means that we have to add an extra dimension with expand_dims (text, -1) to reshape it so that each element is a single string in the batch. We can do this by defining custom vectorize_text function, which we will further be used to vectorize both train and test datasets."
      ],
      "metadata": {
        "id": "TpgboGWVF_iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import expand_dims\n",
        "\n",
        "def vectorize_text(text, label):\n",
        "  text = expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label\n",
        "\n",
        "train_dataset = train_dataset.map(vectorize_text)"
      ],
      "metadata": {
        "id": "R-ELlcu4GPF_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Model\n",
        "After the TextVectorization layer is applied, input text is transformed into a vector of integers, where every entry represents one token (one word in this case). Further, the classification model for sentiment analysis should be defined.\n",
        "\n",
        "The Embedding layer is used to transform the integer-encoded tokens into a set of dense vectors of predefined dimensions (regulated by output_dim parameter). These vectors are learned as the model trains. Further, the set of vectors is averaged using GlobalAveragePooling1D layer to produce a fixed-length output vector suitable for feeding into a dense layer of the neural network. The output dense layer has only one neuron, since output is encoded as a single integer (0 for negative and 1 for\n",
        "positive reviews), with sigmoid activation function that is suitable for this kind of output format. Finally, model is compiled using adam optimizer and binary_crossentropy loss function."
      ],
      "metadata": {
        "id": "T7_MytX1GZX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "def define_model():\n",
        "  model = Sequential()\n",
        "  # Input dim - Maximum size of the vocabulary\n",
        "  # Output dim - Embedding vectors dimension\n",
        "  model.add(layers.Embedding(input_dim=10000, output_dim=16))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.GlobalAveragePooling1D())\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "  # Compile model\n",
        "  model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "uyUmK2yQGk0u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment"
      ],
      "metadata": {
        "id": "tuFA4ZuSKPcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import shutil\n",
        "# from keras.utils import text_dataset_from_directory\n",
        "\n",
        "\n",
        "# Define train path\n",
        "dataset_dir = '/content/drive/MyDrive/Colab Notebooks/aclImdb'\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "\n",
        "# Remove additional unsup/ directory from the train/ directory\n",
        "remove_dir = os.path.join(train_dir, \"unsup\")\n",
        "if os.path.exists(remove_dir):\n",
        "  shutil.rmtree(remove_dir)"
      ],
      "metadata": {
        "id": "ZsHRfenTKR9K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract labels from train dataset\n",
        "train_labels = [label.numpy() for _, label in train_dataset.unbatch()]\n",
        "\n",
        "# Print first 10 labels\n",
        "print(train_labels[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AayceknMFeg",
        "outputId": "a3dab43b-bf24-4b6c-d3ab-f38843f86171"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 1, 1, 1, 0, 1, 1, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.strings import lower, regex_replace\n",
        "\n",
        "\n",
        "print(os.listdir(train_dir))  # Should show 'pos' and 'neg'\n",
        "print(os.listdir(os.path.join(train_dir, \"pos\"))[:5])  # List first 5 positive reviews\n",
        "print(os.listdir(os.path.join(train_dir, \"neg\"))[:5])  # List first 5 negative reviews\n",
        "# print(os.listdir(os.path.join(train_dir, \"neg\")))  # List all negative reviews\n",
        "\n",
        "\n",
        "# Print positive review example\n",
        "pos_file = os.path.join(train_dir, \"pos\", \"5576_9.txt\")  # Adjust filename if needed\n",
        "\n",
        "with open(pos_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    # print(\"Positive review example:\", f.read())\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = lower(text)  # Convert to lowercase\n",
        "    text = regex_replace(text, \"<br />\", \" \")  # Remove HTML tags\n",
        "    text = regex_replace(text, \"[^A-Za-z0-9]+\", \" \")  # Remove non-alphanumeric characters\n",
        "    return text\n",
        "\n",
        "print(\"Positive review example:\", text)\n",
        "processed_text = preprocess(tf.convert_to_tensor(text))\n",
        "print(\"Positive review example preprocessed:\", processed_text.numpy().decode(\"utf-8\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aJpXwOJM_KU",
        "outputId": "3ccc44fb-7fed-4436-e195-e020cb1d5040"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['urls_unsup.txt', 'urls_pos.txt', 'urls_neg.txt', 'labeledBow.feat', 'unsupBow.feat', 'neg', 'pos']\n",
            "['11410_7.txt', '11669_9.txt', '11771_10.txt', '11659_9.txt', '11778_10.txt']\n",
            "['11508_1.txt', '11519_1.txt', '11632_1.txt', '11636_3.txt', '11671_2.txt']\n",
            "Positive review example: This movie is hilarious! I watched it with my friend and we just had to see it again. This movie is not for you movie-goers who will only watch the films that are nominated for Academy Awards (you know who you are.)I won't recap it because you have seen that from all the other reviews.<br /><br />\"Whipped\" is a light-hearted comedy that had me laughing throughout. It doesn't take itself too seriously and should be watched with your friends, not your girlfriend. It won't win any awards, but it just has to be watched to be appreciated. True, some of the jokes are toilet humor, but that is not necessarily a bad thing. Everyone can use some of it sometimes. Some people need to lighten up and see \"Whipped\" for what it is, not what it isn't.<br /><br />****1/4 out of *****.\n",
            "Positive review example preprocessed: this movie is hilarious i watched it with my friend and we just had to see it again this movie is not for you movie goers who will only watch the films that are nominated for academy awards you know who you are i won t recap it because you have seen that from all the other reviews whipped is a light hearted comedy that had me laughing throughout it doesn t take itself too seriously and should be watched with your friends not your girlfriend it won t win any awards but it just has to be watched to be appreciated true some of the jokes are toilet humor but that is not necessarily a bad thing everyone can use some of it sometimes some people need to lighten up and see whipped for what it is not what it isn t 1 4 out of \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Load train and test datasets\n",
        "train_dataset = text_dataset_from_directory(train_dir, batch_size=32, label_mode=\"int\")\n",
        "# test_dataset = text_dataset_from_directory(test_dir, batch_size=32, label_mode=\"int\")\n",
        "\n",
        "# Extract only text data from train_dataset to adapt the TextVectorization layer\n",
        "text_only_train = train_dataset.map(lambda text, label: text)\n",
        "\n",
        "# Adapt the TextVectorization layer\n",
        "vectorize_layer.adapt(text_only_train)\n",
        "\n",
        "# Apply the TextVectorization layer to both datasets\n",
        "train_dataset = train_dataset.map(lambda text, label: (vectorize_layer(text), label))\n",
        "# test_dataset = test_dataset.map(lambda text, label: (vectorize_layer(text), label))\n",
        "\n",
        "# Define the model using the provided function\n",
        "model = define_model()\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    x=train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=train_dataset,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(train_dataset)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Retrieve training history\n",
        "history = model.history\n",
        "\n",
        "# Plot accuracy and loss in one plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\", color=\"blue\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"green\")\n",
        "plt.plot(history.history[\"loss\"], label=\"Training Loss\", linestyle=\"dashed\", color=\"red\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\", linestyle=\"dashed\", color=\"orange\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Accuracy & Loss Over Epochs\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QASzJE5gWdns"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}