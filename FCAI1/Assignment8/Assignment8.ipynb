{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Normalization and Weight Decay\n",
        "Two common regularization techniques in CNNs when working with computer vision tasks are Batch Normalization and Weight Decay. Regularization techniques in CNNs are used to prevent overfitting, improve generalization ability and speed up the training process.\n",
        "<br>Gradient descent<br>\n",
        "Gradient descent is a way to adjust model's weights and biases to find the lowest point of error, helping the model make better guesses when it sees new data.\n",
        "<br>Normalization vs standardization<br>\n",
        "Normalization scales data to a range between 0 and 1, while standardization transforms data using the formula (x - m)/sigma, where x is a data point, m is the mean, and sigma is the standard deviation, resulting in data with a mean of 0 and a standard deviation of 1.<br>\n",
        "Exploding gradients is a problem in neural networks where the gradients (which guide how the model learns) become too large, causing the model to make huge, erratic changes to the weights and making the learning process fail.<br>\n",
        "Stochastic Gradient Descent (SGD) is a variation of gradient descent where the model updates its weights using only one random data point at a time, instead of the entire dataset, making the learning process faster and more efficient for large datasets.<br>\n",
        "Batch normalization normalizes the output of each activation function, ensuring that the data entering the next layer has a consistent scale. This helps speed up training and makes it more stable\n",
        "\n"
      ],
      "metadata": {
        "id": "jA8CvbrBuXJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch normalization layer (BNL)\n",
        "Batch Norm is a neural network layer that is now commonly used in many architectures. It often gets added as part of a Linear or Convolutional block and helps to stabilize the network during training.<br>\n",
        "Batch normalization is used to normalize the input to a specific layer so that the distribution of the activations remains stable during training. By controlling the distribution of activations and gradients, the learning process is stabilized, which leads to improved convergence, enables usage of higher learning rates, and has regularization technique (even though this is not its primary purpose). Since this layer enables the normalization process between two consecutive hidden layers, it is usually placed after the convolutional layers (before or after the activation function).\n",
        "<br>BNL Parameters<br>\n",
        "The batch normalization layer has 4 parameters, 2 trainable during the backpropagation (β and γ - used to shift and scale transformed distribution to ensure better performance of the model for the specific task), and 2 non-trainable (moving average mean and variance - used for normalization). Even though moving average mean and variance are not learned from training, their values are estimated from input data during the training phase, and are stored and used during inference."
      ],
      "metadata": {
        "id": "Ee0VzE-e_Ap-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# BATCH NORMALIZATION LAYER\n",
        "model = models.Sequential()\n",
        "\n",
        "# Remove activation function from the Conv2D layer\n",
        "model.add(Input(shape=(32, 32, 3)))\n",
        "model.add(layers.Conv2D(32, (3, 3), padding=\"same\"))\n",
        "\n",
        "# Insert batch normalization\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "# Add activation function\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "\n",
        "# Remove activation function from the Conv2D layer\n",
        "model.add(layers.Conv2D(32, (3, 3), padding=\"same\"))\n",
        "\n",
        "# Insert batch normalization\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "# Add activation function\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.2))"
      ],
      "metadata": {
        "id": "HvuAvfYk7oRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight decay\n",
        "Weight decay is a regularization technique that penalizes large weights during training. It does this by adding a term to the loss function that encourages smaller values for the model’s weights, thus preventing overfitting. There are multiple version of the weight decay implementation based on the vector norm typed that are added as a penalty to the loss function:\n",
        "\n",
        "1. L1 - Uses L1 vector norm (sum of the absolute weights);\n",
        "2. L2 - Uses L2 vector norm (sum of the squared weights);\n",
        "3. L1L2 - Sum of the absolute and squared weights.\n",
        "\n",
        "By introducing weight decay, we ensure smaller values for the weights, which prevents the model from capturing noise instead of true patterns in the data and improves generalization abilities. Weight decay can be applied on the model’s layers by using the Keras regularizers module."
      ],
      "metadata": {
        "id": "_X93r7AD-fUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.regularizers import l2\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# L2 regularization with 0.001 regularization factor\n",
        "model = models.Sequential()\n",
        "model.add(Input(shape=(32, 32, 3)))\n",
        "model.add(layers.Conv2D(32, (3, 3), padding=\"same\", kernel_regularizer=l2(0.001)))\n",
        "\n",
        "# Batch normalization layer\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "# Activation function\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "\n",
        "# L2 regularization with 0.001 regularization factor\n",
        "# model.add(Input(shape=(32, 32, 3))) # already configured to use input shape (None, 32, 32, 3)\n",
        "model.add(layers.Conv2D(32, (3, 3), padding=\"same\", kernel_regularizer=l2(0.001)))\n",
        "\n",
        "# Batch normalization layer\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "# Activation function\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.2))"
      ],
      "metadata": {
        "id": "RNgi27jO_MhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer learning\n",
        "Transfer learning is a machine learning technique where a model trained on one task (typically with a large dataset) is reused or \"fine-tuned\" to solve a different, but related, task. The key idea is that knowledge gained from solving one problem can be transferred to help solve a different problem more efficiently, particularly when the new task has limited data. In the context of Convolutional Neural Networks (CNNs), transfer learning leverages pre-trained models (typically trained on large, general-purpose datasets like ImageNet) and adapts them for specific tasks. Some popular pre-trained models for computer vision tasks are VGG16 (or VGG19), ResNet, Inception and MobileNet.<br>\n",
        "CNN architecture can be divided into two main parts:\n",
        "1. Feature extraction layers - Typically consisted of convolutional and pooling layers.\n",
        "2. Classification layers - Dense (fully connected) layers.\n",
        "\n",
        "Considering that, pre-trained models can be used in several different ways:\n",
        "1. One-shot classification - Already pre-trained model is loaded and used for custom task prediction without additional training.\n",
        "2. Transfer learning - Feature extraction layers from pre-trained model are loaded and \"freezed\" so their weights cannot be modified during the training process, while newly created dense layers are added on top of them, which will be trained for custom task on a new dataset.\n",
        "3. Fine-tuning - Feature extraction layers from pre-trained model are loaded and trained along with the classification layers using a small learning rate for a new custom task.\n"
      ],
      "metadata": {
        "id": "He15V9F6A97u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural network VGG16\n",
        "VGG16 is a 16-layer deep neural network with a straightforward architecture. It exclusively uses 3x3 convolutional filters with a stride of 1, and 2x2 max-pooling layers with a stride of 2, arranged in 5 convolutional blocks (with various number of Conv2D layers, MaxPooling layer and ReLU activation function). It was originally trained on 224x224x3 size images.\n",
        "https://keras.io/api/applications/vgg/#vgg16-function"
      ],
      "metadata": {
        "id": "C9vtFvG4Ckq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# Load the VGG16 model which is pre-trained on ImageNet data and exclude top layer\n",
        "# When loading the VGG16 model, the input_shape parameter should be (32, 32, 3) to match the shape of\n",
        "# the CIFAR-10 images, since the original VGG16 model was trained on images with different resolution (224x224x3)\n",
        "vgg_base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "# Freeze the VGG16 model layers to prevent training them\n",
        "vgg_base_model.trainable = False\n",
        "\n",
        "# On top of the VGG16 feature extraction model, we can add fully connected layers for classification purpose\n",
        "model = models.Sequential()\n",
        "\n",
        "# Add the base VGG16 model\n",
        "model.add(vgg_base_model)\n",
        "\n",
        "# Add fully connected layers on top of VGG16 base model\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Finally, the code for loading and unfreezing certain VGG16 layers is provided\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n",
        "base_model.trainable = False\n",
        "set_trainable = False\n",
        "\n",
        "# Unfreeze layers from block5_conv1 onwards\n",
        "for layer in base_model.layers:\n",
        "  if layer.name == \"block5_conv1\":\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O84wVHboCwzx",
        "outputId": "805328e4-b15d-49dc-d6b1-9b3d0d27c61b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MaxPooling2D name=block5_pool, built=True>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 1\n",
        "1. In the final model from the last lab exercise (Figure 4), insert a batch normalization layer between every convolutional layer and activation function. Also, add batch normalization between the first dense layer and\n",
        "activation function."
      ],
      "metadata": {
        "id": "PNN50JPREW6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import Input\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "(trainX, trainy), (testX, testy) = cifar10.load_data() # load data\n",
        "\n",
        "# One-hot encoding for labels\n",
        "trainy = to_categorical(trainy, 10)\n",
        "testy = to_categorical(testy, 10)\n",
        "\n",
        "# Convert images to float32 and scale them\n",
        "trainX = trainX.astype('float32') / 255.0\n",
        "testX = testX.astype('float32') / 255.0\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First Conv2D Block\n",
        "model.add(Input(shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(MaxPooling2D((3, 3)))\n",
        "model.add(Dropout(0.2))  # Dropout after first MaxPooling2D\n",
        "\n",
        "# Second Conv2D Block\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(MaxPooling2D((3, 3)))\n",
        "model.add(Dropout(0.2))  # Dropout after second MaxPooling2D\n",
        "\n",
        "# Third Conv2D Block (newly added)\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(MaxPooling2D((3, 3)))\n",
        "model.add(Dropout(0.2))  # Dropout after third MaxPooling2D\n",
        "\n",
        "# Flatten Layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense Layer with Batch Normalization\n",
        "model.add(Dense(128))\n",
        "model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "\n",
        "# Dropout after Dense layer\n",
        "model.add(Dropout(0.2))  # Dropout after first Dense layer\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(trainX, trainy, epochs=5, batch_size=64, validation_data=(testX, testy), verbose=1)\n",
        "\n",
        "# Print accuracy vs loss\n",
        "for epoch in range(5):\n",
        "    print(f\"Epoch {epoch + 1}: Accuracy = {history.history['accuracy'][epoch]}, Loss = {history.history['loss'][epoch]}\")\n",
        "\n",
        "# Print final loss and accuracy\n",
        "print(f\"\\nFinal Loss: {history.history['loss'][-1]}, Final Accuracy: {history.history['accuracy'][-1]}\")\n"
      ],
      "metadata": {
        "id": "PVVwYsjdFBwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the accuracy vs loss\n",
        "Plot the behavior of accuracy and loss over the epochs.\n",
        "1. What can you conclude from the plots?"
      ],
      "metadata": {
        "id": "o9JcsgruRy7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy and loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.title('Training Accuracy and Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zlbJSG3ZSA55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reduce the batch normalization momentum\n",
        "Instability in validation accuracy is a common issue that can arise due to interactions between batch normalization and other training setup (e.g., optimizer, learning rate). The easiest solution is to reduce the batch normalization’s momentum (default 0.99), while keeping our base architecture the same."
      ],
      "metadata": {
        "id": "sS40zr7LSDzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import Input\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "(trainX, trainy), (testX, testy) = cifar10.load_data() # load data\n",
        "\n",
        "# One-hot encoding for labels\n",
        "trainy = to_categorical(trainy, 10)\n",
        "testy = to_categorical(testy, 10)\n",
        "\n",
        "# Convert images to float32 and scale them\n",
        "trainX = trainX.astype('float32') / 255.0\n",
        "testX = testX.astype('float32') / 255.0\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First Conv2D Block\n",
        "model.add(Input(shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "# model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "# model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(MaxPooling2D((3, 3)))\n",
        "model.add(Dropout(0.2))  # Dropout after first MaxPooling2D\n",
        "\n",
        "# Second Conv2D Block\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "# model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "# model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(MaxPooling2D((3, 3)))\n",
        "model.add(Dropout(0.2))  # Dropout after second MaxPooling2D\n",
        "\n",
        "# Third Conv2D Block (newly added)\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "# model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "# model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(MaxPooling2D((3, 3)))\n",
        "model.add(Dropout(0.2))  # Dropout after third MaxPooling2D\n",
        "\n",
        "# Flatten Layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense Layer with Batch Normalization\n",
        "model.add(Dense(128))\n",
        "#model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "\n",
        "# Dropout after Dense layer\n",
        "model.add(Dropout(0.2))  # Dropout after first Dense layer\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(trainX, trainy, epochs=5, batch_size=64, validation_data=(testX, testy), verbose=1)\n",
        "\n",
        "# Print accuracy vs loss\n",
        "for epoch in range(5):\n",
        "    print(f\"Epoch {epoch + 1}: Accuracy = {history.history['accuracy'][epoch]}, Loss = {history.history['loss'][epoch]}\")\n",
        "\n",
        "# Print final loss and accuracy\n",
        "print(f\"\\nFinal Loss: {history.history['loss'][-1]}, Final Accuracy: {history.history['accuracy'][-1]}\")\n",
        "\n",
        "# Plot accuracy and loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.title('Training Accuracy and Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7eBQvtWnS12o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding weight decay\n",
        "In addition to the previously added batch normalization (with momentum 0.8), add weight decay. More precisely, add the L2 regularizer with regularization factor of 0.001 to every Conv2D and first Dense layer."
      ],
      "metadata": {
        "id": "imCGaWtHUIQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import Input\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "(trainX, trainy), (testX, testy) = cifar10.load_data() # load data\n",
        "\n",
        "# One-hot encoding for labels\n",
        "trainy = to_categorical(trainy, 10)\n",
        "testy = to_categorical(testy, 10)\n",
        "\n",
        "# Convert images to float32 and scale them\n",
        "trainX = trainX.astype('float32') / 255.0\n",
        "testX = testX.astype('float32') / 255.0\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First Conv2D Block\n",
        "model.add(Input(shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(MaxPooling2D((3, 3)))\n",
        "model.add(Dropout(0.2))  # Dropout after first MaxPooling2D\n",
        "\n",
        "# Second Conv2D Block\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(MaxPooling2D((3, 3)))\n",
        "model.add(Dropout(0.2))  # Dropout after second MaxPooling2D\n",
        "\n",
        "# Third Conv2D Block (newly added)\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())  # Batch Normalization\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "model.add(MaxPooling2D((3, 3)))\n",
        "model.add(Dropout(0.2))  # Dropout after third MaxPooling2D\n",
        "\n",
        "# Flatten Layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense Layer with Batch Normalization\n",
        "model.add(Dense(128, kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization(momentum=0.8)) # Batch Normalization Reduced\n",
        "model.add(Activation('relu'))    # Activation after Batch Normalization\n",
        "\n",
        "# Dropout after Dense layer\n",
        "model.add(Dropout(0.2))  # Dropout after first Dense layer\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(trainX, trainy, epochs=5, batch_size=64, validation_data=(testX, testy), verbose=1)\n",
        "\n",
        "# Print accuracy vs loss\n",
        "for epoch in range(5):\n",
        "    print(f\"Epoch {epoch + 1}: Accuracy = {history.history['accuracy'][epoch]}, Loss = {history.history['loss'][epoch]}\")\n",
        "\n",
        "# Print final loss and accuracy\n",
        "print(f\"\\nFinal Loss: {history.history['loss'][-1]}, Final Accuracy: {history.history['accuracy'][-1]}\")\n",
        "\n",
        "# Plot accuracy and loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.title('Training Accuracy and Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OQfTsKGXUSis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model comparison\n",
        "Compare the accuracy obtained by using these three models. The expected accuracy per model type is:\n",
        "\n",
        "Baseline model 73 - 76%\n",
        "Baseline model + BN 80 - 82%\n",
        "Baseline model + BN + L2 83 - 85%\n",
        "\n",
        "1. Explain the increase in accuracy after every modification."
      ],
      "metadata": {
        "id": "kNikGuBmWeZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing batch normalization, momentum and l2\n",
        "Try different values for the batch normalization momentum and l2 parameters. Record the behavior of these models. Did you achieve better performance, and using which values of these parameters?"
      ],
      "metadata": {
        "id": "awC2googW7SW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 2"
      ],
      "metadata": {
        "id": "71tdSmyKXWam"
      }
    }
  ]
}