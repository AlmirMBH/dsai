{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Course: Fundamental Concepts of AI**\n",
        "\n",
        "#**Homework 3: Applying ML Algorithms**\n",
        "\n"
      ],
      "metadata": {
        "id": "MQtLIqo7RhZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VjHnEmKjkQl",
        "outputId": "ea681b20-fd7a-4d02-d4ed-ee90158c2e9d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Student name and surname:**\n",
        "Almir Mustafic\n",
        "**Student index:**\n",
        "20114\n",
        "**Date:** January 3, 2025"
      ],
      "metadata": {
        "id": "MhYVVX6IR-G_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Objective:\n",
        "In this assignment, you will apply the machine learning models introduced in Labs 5 and 6 to the dataset you chose and analyzed in Homework 2 (Assignment 3). Your goal is to train and evaluate these models on your dataset and compare their performance.\n",
        "\n",
        "---\n",
        "\n",
        "## Tasks:\n",
        "\n",
        "### 1. Data Preparation and Preprocessing\n",
        "Prepare your dataset for machine learning by completing the following steps:\n",
        "\n",
        "1. **Load the Dataset**  \n",
        "   - Use the dataset you analyzed in Homework 2.\n",
        "\n",
        "2. **Inspect the Data**  \n",
        "   - Display the first few rows, column names, and data types.  \n",
        "   - Check for missing values and handle them if necessary (if you haven't already).\n",
        "\n",
        "3. **Split the Data**  \n",
        "   - Split the dataset into training, validation and testing sets (e.g., 70% training, 20% validating, 10% testing).\n",
        "\n",
        "4. **Standardize the Features**  \n",
        "   - Normalize all numerical features so that they have a mean of 0 and a standard deviation of 1 (if you haven't already).  \n",
        "   - Skip this step for categorical features or the target variable.\n",
        "\n",
        "5. **Encode the Target Variable**  \n",
        "   - Ensure that the target column is in a numeric format if necessary (e.g., for classification tasks).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Applying ML Models\n",
        "Train and evaluate the following machine learning models on your dataset. For each model:\n",
        "\n",
        "1. **K-Nearest Neighbors (KNN)**  \n",
        "   - Experiment with a few values of `k`.  \n",
        "   - Report the accuracy on the test set for each experiment.\n",
        "   - Note: Perform this task if you dataset involves a classification problem.\n",
        "\n",
        "2. **Decision Tree (DT)**  \n",
        "   - Train a Decision Tree model with default settings.  \n",
        "   - Report the accuracy on the test set.\n",
        "   - Note: Perform this task if you dataset involves a classification problem.\n",
        "\n",
        "\n",
        "3. **Support Vector Classifier (SVC)**  \n",
        "   - Train a Support Vector Classifier model with default settings.  \n",
        "   - Report the accuracy on the test set.\n",
        "   - Note: Perform this task if you dataset involves a classification problem.\n",
        "\n",
        "\n",
        "4. **Logistic Regression (LR)**  \n",
        "   - Train a Logistic Regression model.  \n",
        "   - Report the accuracy on the test set.\n",
        "   - Note: Only perform this task if your chosen dataset is a binary classification problem (i.e., it has exactly two classes).\n",
        "\n",
        "5. **Neural Network (NN)**  \n",
        "   - Build and train a simple Neural Network.  \n",
        "   - Report the accuracy on the test set.\n",
        "   - Note: Perform this task if you dataset involves a classification or regression problem.\n",
        "\n",
        "\n",
        "6. **Linear Regression (LinR)**\n",
        "  - Train a Linear Regression model on your dataset.\n",
        "  - Report the Mean Squared Error (MSE) on the test set.\n",
        "  - Note: Perform this task only if your dataset involves a regression problem (predicting continuous values).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Model Comparison\n",
        "1. Create a table summarizing the test set accuracy of all models (KNN, DT, SVC, LR, NN and LinR).  \n",
        "2. Discuss the following:\n",
        "   - Which model performed the best?  \n",
        "   - Why do you think this model performed better than the others?  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. Save and Submit\n",
        "1. **Save Your Work:**  \n",
        "   - Save your trained models using an appropriate method (e.g., pickle for Scikit-learn models or `model.save` for Keras models).  \n",
        "   - Save the final notebook with all outputs, tables, discussions included.\n",
        "\n",
        "2. **Submit:**  \n",
        "   - A Colab Notebook to c3 Homework 3 Assignment section.  \n",
        "---\n",
        "\n",
        "## Grading Criteria:\n",
        "- **Data Preparation (2 points)**  \n",
        "- **Implementation of ML Models (2 points)**  \n",
        "- **Comparison and Analysis (1 point)**  \n",
        "- **Code Clarity and Documentation (1 point)**\n"
      ],
      "metadata": {
        "id": "LkF50gGAXLCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1.1) Load the dataset\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/penguins.csv'\n",
        "penguins_data = pd.read_csv(file_path)\n",
        "\n",
        "# 1.2.1) Display the first few rows, column names, and data types.\n",
        "print(penguins_data.head())\n",
        "print(\"Column names:\", penguins_data.columns)\n",
        "print(\"Data types:\", penguins_data.dtypes)\n",
        "\n",
        "# 1.2.2) Check for missing values and handle them\n",
        "# Replace the missing values in numerical columns with a median value\n",
        "# Replace the missing values in categorical columns with the most common value\n",
        "# Check if there are missing values\n",
        "print(\"\\nMissing values handling\")\n",
        "print(penguins_data.isnull().sum())\n",
        "numerical_columns = penguins_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "penguins_data[numerical_columns] = penguins_data[numerical_columns].fillna(penguins_data[numerical_columns].median())\n",
        "categorical_columns = penguins_data.select_dtypes(include=['object']).columns\n",
        "\n",
        "for col in categorical_columns: # 'species', 'island', 'sex'\n",
        "    most_common_value = penguins_data[col].mode()[0] # if more than 1 take the first\n",
        "    penguins_data[col] = penguins_data[col].fillna(most_common_value) # replace empty with the most common one\n",
        "\n",
        "# Check if there are any remaining missing values\n",
        "# print(penguins_data.isnull().sum())\n",
        "\n",
        "# 1.3) Split the dataset into training, validation and testing sets (e.g., 70% training, 20% validating, 10% testing)\n",
        "# The number 42 is famously used as a \"random\" choice due to its cultural reference in The Hitchhiker's Guide to\n",
        "# the Galaxy, where it is the answer to the \"Ultimate Question of Life, the Universe, and Everything.\"\n",
        "train_data, remainig_data_data = train_test_split(penguins_data, test_size=0.3, random_state=42)\n",
        "validation_data, test_data = train_test_split(remainig_data_data, test_size=0.33, random_state=42)\n",
        "# print(\"Training data:\", train_data.shape)\n",
        "# print(\"Validation data:\", validation_data.shape)\n",
        "# print(\"Test data:\", test_data.shape)\n",
        "\n",
        "# 1.4) Normalize all numerical features so that they have a mean of 0 and a standard deviation of 1\n",
        "# 1.4.1) Skip this step for categorical features or the target variable.\n",
        "# Normalized Value = (Value−Mean)/Standard Deviation\n",
        "# Standard deviation = square root of the average of the squared differences between each number and the mean of the data\n",
        "numerical_columns = penguins_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "scaler = StandardScaler()\n",
        "penguins_data[numerical_columns] = scaler.fit_transform(penguins_data[numerical_columns])\n",
        "# print(penguins_data.head())\n",
        "\n",
        "# 1.5) Convert the target column to numeric (species)\n",
        "unique_species = penguins_data['species'].unique()\n",
        "penguins_data['species'] = penguins_data['species'].astype('category').cat.codes # .cat.codes converts categorical values in a pandas category dtype column into numeric codes\n",
        "unique_species_numerical = penguins_data['species'].unique()\n",
        "# print(unique_species)\n",
        "# print(unique_species_numerical)\n",
        "\n",
        "\n",
        "# Prepare data for training and testing\n",
        "# Take the labels i.e. categories and training, validation and test data without species column\n",
        "X_train = train_data.drop(columns=['species'])\n",
        "X_validation = validation_data.drop(columns=['species'])\n",
        "X_test = test_data.drop(columns=['species'])\n",
        "y_train = train_data['species']\n",
        "y_validation = validation_data['species']\n",
        "y_test = test_data['species']\n",
        "\n",
        "# Convert categorical columns into numeric codes\n",
        "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
        "for col in categorical_columns:\n",
        "    X_train[col] = X_train[col].astype('category').cat.codes\n",
        "    X_validation[col] = X_validation[col].astype('category').cat.codes\n",
        "    X_test[col] = X_test[col].astype('category').cat.codes\n",
        "\n",
        "\n",
        "# 2) K-Nearest Neighbors (KNN)\n",
        "# 2.1.1) Experiment with a few values of k\n",
        "# 2.1.2.) Report the accuracy on the test set for each experiment\n",
        "# 2.1.3) Note: Perform this task if your dataset involves a classification problem\n",
        "print(\"K-NEAREST NEIGHBORS (KNN)\")\n",
        "print(\"Testing k values on validation data set\")\n",
        "best_k = None\n",
        "best_accuracy = 0\n",
        "k_values = [3, 17, 65, 66, 99]\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_val_pred = knn.predict(X_validation)\n",
        "    validation_accuracy = accuracy_score(y_validation, y_val_pred)\n",
        "    print(f\"k={k} - accuracy={validation_accuracy:.3f}\")\n",
        "\n",
        "    if validation_accuracy > best_accuracy:\n",
        "        best_k = k\n",
        "        best_accuracy = validation_accuracy\n",
        "\n",
        "print(f\"Best-performing k is {best_k}\")\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=best_k)\n",
        "knn.fit(X_train, y_train)\n",
        "y_test_pred = knn.predict(X_test)\n",
        "test_accuracy = knn_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"The test data set accuracy after training and for best k is {test_accuracy:.3f}\")\n",
        "print(f\"Test vs validation data performance difference: {test_accuracy - best_accuracy:.3f}\")\n",
        "print(f\"\\nKNN Neighbors Accuracy: {test_accuracy:.3f}\")\n",
        "print(\"==============================================\")\n",
        "\n",
        "\n",
        "# 2.2) Decision Tree (DT)\n",
        "# 2.2.1) Train a Decision Tree model with default settings\n",
        "# 2.2.2) Report the accuracy on the test set\n",
        "# 2.2.3) Note: Perform this task if you dataset involves a classification problem\n",
        "# The max_depth_value is a the maximum depth for the Decision Tree i.e. it controls how deep the tree can grow.\n",
        "# Smaller values limit the tree's depth, reducing overfitting but might lead to underfitting.\n",
        "# Larger values grow the tree deeper including more patterns but might lead to overfitting. None means unlimited.\n",
        "print(\"\\nDECISION TREE (DT)\")\n",
        "max_depth_values = [None, 1, 5, 10, 20]\n",
        "decision_tree = None\n",
        "best_accuracy = 0\n",
        "\n",
        "print(\"Validation after training accuracy\")\n",
        "for max_depth in max_depth_values:\n",
        "    dt_model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
        "    dt_model.fit(X_train, y_train)\n",
        "    y_val_pred = dt_model.predict(X_validation)\n",
        "    val_accuracy = accuracy_score(y_validation, y_val_pred)\n",
        "    print(f\"max_depth={max_depth} - accuracy={val_accuracy:.3f}\")\n",
        "\n",
        "    if val_accuracy > best_accuracy:\n",
        "        decision_tree = dt_model\n",
        "        best_accuracy = val_accuracy\n",
        "\n",
        "y_test_pred = decision_tree.predict(X_test)\n",
        "test_accuracy = decision_tree_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"The test data set accuracy after training and for best k is {test_accuracy:.3f}\")\n",
        "print(f\"Test vs validation data performance difference: {test_accuracy - best_accuracy:.3f}\")\n",
        "print(f\"\\nDecision Tree Accuracy: {test_accuracy:.3f}\")\n",
        "print(\"==============================================\")\n",
        "\n",
        "\n",
        "# 2.3) Support Vector Classifier (SVC)\n",
        "# 2.3.1) Train a Support Vector Classifier model with default settings.\n",
        "# 2.3.2) Report the accuracy on the test set.\n",
        "# 2.3.3) Note: Perform this task if you dataset involves a classification problem.\n",
        "print(\"\\nSUPPORT VECTOR CLASSIFIER (SVC)\")\n",
        "# The goal of SVC is to maximize the margin between classes while minimizing the classification error.\n",
        "# The C parameter controls the trade-off between these two objectives.\n",
        "# High C means the model will be very strict about fitting the training data (fewer misclassifications), but it might lead to overfitting.\n",
        "# Lower C means the model will tolerate more misclassifications, which might lead to a wider margin and more generalization, but also to\n",
        "# less accuracy on the training data.\n",
        "C_values = [0.1, 1, 10, 100]\n",
        "best_accuracy = 0\n",
        "best_C = None\n",
        "support_vector_classifier = None\n",
        "\n",
        "for C in C_values:\n",
        "    svc_model = SVC(C=C, random_state=42)\n",
        "    svc_model.fit(X_train, y_train)\n",
        "    y_pred_validation = svc_model.predict(X_validation)\n",
        "    accuracy = accuracy_score(y_validation, y_pred_validation)\n",
        "    print(f\"max_depth={C} - accuracy={accuracy:.3f}\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_C = C\n",
        "        support_vector_classifier = svc_model\n",
        "\n",
        "y_pred_test = support_vector_classifier.predict(X_test)\n",
        "test_accuracy = svc_test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "print(f\"The test data set accuracy after training, validation and for best C is {best_accuracy:.3f}\")\n",
        "print(f\"Test vs validation data performance difference: {test_accuracy - best_accuracy:.3f}\")\n",
        "print(f\"\\nSVC Accuracy: {test_accuracy:.3f}\")\n",
        "print(\"====================================\")\n",
        "\n",
        "\n",
        "# 2.4) Logistic Regression (LR)\n",
        "# 2.4.1) Train a Logistic Regression model.\n",
        "# 2.4.2) Report the accuracy on the test set.\n",
        "# 2.4.3) Note: Only perform this task if your chosen dataset is a binary classification problem (i.e., it has exactly two classes).\n",
        "# NOTE: My data set has more classes, but I decided to change the classification task to 'sex' instead od species, so that I can do this\n",
        "# task out of curiosity and future reference. If I should not have done this, please ignore it.\n",
        "print(\"\\nLOGISTIC REGRESSION\")\n",
        "penguins_binary_sex = penguins_data.dropna(subset=['sex'])\n",
        "penguins_binary_sex['sex'] = penguins_binary_sex['sex'].str.lower()\n",
        "penguins_binary_sex['sex'] = penguins_binary_sex['sex'].map({'male': 1, 'female': 0})\n",
        "\n",
        "X_lr = penguins_binary_sex.drop('sex', axis=1)\n",
        "y_lr = penguins_binary_sex['sex']\n",
        "\n",
        "non_numeric_columns = X_lr.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "for col in non_numeric_columns:\n",
        "    most_common_value = X_lr[col].mode()[0] # Fill missing values with the most common value (mode)\n",
        "    X_lr[col] = X_lr[col].fillna(most_common_value)\n",
        "\n",
        "X_lr = X_lr.apply(pd.to_numeric, errors='coerce') # Convert to numeric\n",
        "X_lr = X_lr.fillna(0)  # Remove the remaining NaN values, if any present\n",
        "\n",
        "X_lr_train, X_lr_temp, y_lr_train, y_lr_temp = train_test_split(X_lr, y_lr, test_size=0.4, random_state=42)\n",
        "X_lr_val, X_lr_test, y_lr_val, y_lr_test = train_test_split(X_lr_temp, y_lr_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_lr_train, y_lr_train)\n",
        "y_val_pred = model.predict(X_lr_val)\n",
        "y_test_pred = model.predict(X_lr_test)\n",
        "validation_accuracy = accuracy_score(y_lr_val, y_val_pred)\n",
        "test_accuracy = accuracy_score(y_lr_test, y_test_pred)\n",
        "\n",
        "print(f\"Validation Accuracy: {validation_accuracy:.3f}\")\n",
        "print(f\"Test vs validation data performance difference: {test_accuracy - validation_accuracy:.3f}\")\n",
        "print(f\"\\nLR Accuracy: {test_accuracy:.3f}\")\n",
        "print(\"===================================\")\n",
        "\n",
        "\n",
        "# 2.5) Neural Network (NN)\n",
        "# 2.5.1) Build and train a simple Neural Network.\n",
        "# 2.5.2) Report the accuracy on the test set.\n",
        "# 2.5.3) Note: Perform this task if you dataset involves a classification or regression problem.\n",
        "print(\"\\nNEURAL NETWORK (NN)\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_validation_scaled = scaler.transform(X_validation)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Encode target labels and convert data to arrays\n",
        "label_encoder = {label: idx for idx, label in enumerate(y_train.unique())}\n",
        "y_train_encoded = y_train.map(label_encoder)\n",
        "y_validation_encoded = y_validation.map(label_encoder)\n",
        "y_test_encoded = y_test.map(label_encoder)\n",
        "\n",
        "X_train_np = np.array(X_train_scaled)\n",
        "X_validation_np = np.array(X_validation_scaled)\n",
        "X_test_np = np.array(X_test_scaled)\n",
        "y_train_np = np.array(y_train_encoded)\n",
        "y_validation_np = np.array(y_validation_encoded)\n",
        "y_test_np = np.array(y_test_encoded)\n",
        "\n",
        "neural_network = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(X_train_np.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(label_encoder), activation='softmax')\n",
        "])\n",
        "\n",
        "# sparse_categorical_crossentropy used when target labels are integers\n",
        "# categorical_crossentropy used when target labels are one-hot encoded\n",
        "# Verbose: 0 - no output, 1 - displays a progress, 2 - displays one line per epoch with training details\n",
        "# validation_split=0.2 splits the X_train data into training and validation, if no explicit validation data to pass like here\n",
        "# early_stopping is a callback for early stopping based on validation loss to prevent overfitting\n",
        "neural_network.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "history = neural_network.fit(X_train_np, y_train_np, validation_data=(X_validation_np, y_validation_np), epochs=50, batch_size=32, verbose=0, callbacks=[early_stopping])\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "train_accuracy = history.history['accuracy'][-1]\n",
        "loss, accuracy = neural_network.evaluate(X_test_np, y_test_np)\n",
        "nn_test_accuracy = accuracy\n",
        "print(f\"Train Accuracy: {train_accuracy:.3f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.3f}\")\n",
        "print(f\"Test vs validation data performance difference: {accuracy - val_accuracy:.3f}\")\n",
        "print(f\"\\nNN Accuracy: {accuracy:.3f}\\nTest Loss: {loss:.3f}\")\n",
        "print(\"=======================================================\")\n",
        "\n",
        "\n",
        "# 2.6) Linear Regression (LinR)\n",
        "# 2.6.1) Train a Linear Regression model on your dataset.\n",
        "# 2.6.2) Report the Mean Squared Error (MSE) on the test set.\n",
        "# 2.6.3) Note: Perform this task only if your dataset involves a regression problem (predicting continuous values).\n",
        "# NOTE: My data set is based on classes, but I decided to do this out of curiosity and for future reference. I predict the body mass based on\n",
        "# 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'. If I should not have done this, please ignore it.\n",
        "print(\"\\nLINEAR REGRESSION (LN)\")\n",
        "X = penguins_data[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']]\n",
        "y = penguins_data['body_mass_g']  # Target\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_val_pred = model.predict(X_validation)\n",
        "y_test_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_test_pred)\n",
        "r2_val_accuracy = r2_score(y_validation, y_val_pred)\n",
        "r2_test_accuracy = r2_score(y_test, y_test_pred)\n",
        "print(f\"Validation Accuracy: {r2_val_accuracy:.3f}\")\n",
        "# print(f\"Loss: {np.sqrt(mse):.3f}\")\n",
        "print(f\"Test vs validation data performance difference: {r2_test_accuracy - r2_val_accuracy:.3f}\")\n",
        "print(f\"\\nLR Accuracy): {r2_test_accuracy:.3f}\")\n",
        "print(\"=======================================\")\n",
        "\n",
        "\n",
        "# 3) Model Comparison\n",
        "print(\"\\nACCURACIES\")\n",
        "model_comparison = {\n",
        "    'Model': ['KNN', 'Decision Tree', 'SVC', 'Neural Network'],\n",
        "    'Test Accuracy': [knn_test_accuracy, decision_tree_test_accuracy, svc_test_accuracy, nn_test_accuracy]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(model_comparison)\n",
        "print(comparison_df)\n",
        "\n",
        "# 3.2.1) Which model performed the best?\n",
        "# The Neural Network performed the best with a test accuracy of 1.000000, although this was probably the most difficult model to set and train.\n",
        "# Determining the number of layers, neurons, epochs, batch_size, etc. to get a decent accuracy was really challenging.\n",
        "\n",
        "# 3.2.2) Why This Model Performed Better?\n",
        "# Because it took me hours to set it and train it, just kidding :)\n",
        "# Neural Networks can handle complex patterns in data due to their multiple layers and non-linear activation functions.\n",
        "# This is why they are more flexible in learning from the data compared to other models, that is they can adapt to various types of\n",
        "# data relationships and achieve better performance on large datasets.\n",
        "\n",
        "\n",
        "# 4) Save models\n",
        "# 4.1) Save your trained models using an appropriate method (e.g., pickle for Scikit-learn models or model.save for Keras models)\n",
        "# 4.2) Save the final notebook with all outputs, tables, discussions included\n",
        "pickle.dump(knn, open('/content/drive/MyDrive/Colab Notebooks/trained_models/knn_model.pkl', 'wb'))\n",
        "pickle.dump(decision_tree, open('/content/drive/MyDrive/Colab Notebooks/trained_models/decision_tree_model.pkl', 'wb'))\n",
        "pickle.dump(support_vector_classifier, open('/content/drive/MyDrive/Colab Notebooks/trained_models/support_vector_classifier_model.pkl', 'wb'))\n",
        "pickle.dump(neural_network, open('/content/drive/MyDrive/Colab Notebooks/trained_models/neural_network_model.pkl', 'wb'))\n",
        "\n",
        "# Load models\n",
        "knn = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/trained_models/knn_model.pkl', 'rb'))\n",
        "decision_tree = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/trained_models/decision_tree_model.pkl', 'rb'))\n",
        "decision_tree = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/trained_models/support_vector_classifier_model.pkl', 'rb'))\n",
        "decision_tree = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/trained_models/neural_network_model.pkl', 'rb'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqken1TOyJ5D",
        "outputId": "d03469c0-04d4-47f4-8b73-82dc4693169d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "   rowid species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
            "0      1  Adelie  Torgersen            39.1           18.7              181.0   \n",
            "1      2  Adelie  Torgersen            39.5           17.4              186.0   \n",
            "2      3  Adelie  Torgersen            40.3           18.0              195.0   \n",
            "3      4  Adelie  Torgersen             NaN            NaN                NaN   \n",
            "4      5  Adelie  Torgersen            36.7           19.3              193.0   \n",
            "\n",
            "   body_mass_g     sex  year  \n",
            "0       3750.0    male  2007  \n",
            "1       3800.0  female  2007  \n",
            "2       3250.0  female  2007  \n",
            "3          NaN     NaN  2007  \n",
            "4       3450.0  female  2007  \n",
            "Column names: Index(['rowid', 'species', 'island', 'bill_length_mm', 'bill_depth_mm',\n",
            "       'flipper_length_mm', 'body_mass_g', 'sex', 'year'],\n",
            "      dtype='object')\n",
            "Data types: rowid                  int64\n",
            "species               object\n",
            "island                object\n",
            "bill_length_mm       float64\n",
            "bill_depth_mm        float64\n",
            "flipper_length_mm    float64\n",
            "body_mass_g          float64\n",
            "sex                   object\n",
            "year                   int64\n",
            "dtype: object\n",
            "\n",
            "Missing values handling\n",
            "rowid                 0\n",
            "species               0\n",
            "island                0\n",
            "bill_length_mm        2\n",
            "bill_depth_mm         2\n",
            "flipper_length_mm     2\n",
            "body_mass_g           2\n",
            "sex                  11\n",
            "year                  0\n",
            "dtype: int64\n",
            "K-NEAREST NEIGHBORS (KNN)\n",
            "Testing k values on validation data set\n",
            "k=3 - accuracy=0.957\n",
            "k=17 - accuracy=0.928\n",
            "k=65 - accuracy=0.783\n",
            "k=66 - accuracy=0.768\n",
            "k=99 - accuracy=0.754\n",
            "Best-performing k is 3\n",
            "The test data set accuracy after training and for best k is 0.886\n",
            "Test vs validation data performance difference: -0.071\n",
            "\n",
            "KNN Neighbors Accuracy: 0.886\n",
            "==============================================\n",
            "\n",
            "DECISION TREE (DT)\n",
            "Validation after training accuracy\n",
            "max_depth=None - accuracy=1.000\n",
            "max_depth=1 - accuracy=0.826\n",
            "max_depth=5 - accuracy=1.000\n",
            "max_depth=10 - accuracy=1.000\n",
            "max_depth=20 - accuracy=1.000\n",
            "The test data set accuracy after training and for best k is 0.971\n",
            "Test vs validation data performance difference: -0.029\n",
            "\n",
            "Decision Tree Accuracy: 0.971\n",
            "==============================================\n",
            "\n",
            "SUPPORT VECTOR CLASSIFIER (SVC)\n",
            "max_depth=0.1 - accuracy=0.739\n",
            "max_depth=1 - accuracy=0.754\n",
            "max_depth=10 - accuracy=0.913\n",
            "max_depth=100 - accuracy=0.986\n",
            "The test data set accuracy after training, validation and for best C is 0.986\n",
            "Test vs validation data performance difference: -0.014\n",
            "\n",
            "SVC Accuracy: 0.971\n",
            "====================================\n",
            "\n",
            "LOGISTIC REGRESSION\n",
            "Validation Accuracy: 0.884\n",
            "Test vs validation data performance difference: -0.029\n",
            "\n",
            "LR Accuracy: 0.855\n",
            "===================================\n",
            "\n",
            "NEURAL NETWORK (NN)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0033\n",
            "Train Accuracy: 1.000\n",
            "Validation Accuracy: 1.000\n",
            "Test vs validation data performance difference: 0.000\n",
            "\n",
            "NN Accuracy: 1.000\n",
            "Test Loss: 0.003\n",
            "=======================================================\n",
            "\n",
            "LINEAR REGRESSION (LN)\n",
            "Validation Accuracy: 0.722\n",
            "Test vs validation data performance difference: -0.045\n",
            "\n",
            "LR Accuracy): 0.677\n",
            "=======================================\n",
            "\n",
            "ACCURACIES\n",
            "            Model  Test Accuracy\n",
            "0             KNN       0.885714\n",
            "1   Decision Tree       0.971429\n",
            "2             SVC       0.971429\n",
            "3  Neural Network       1.000000\n"
          ]
        }
      ]
    }
  ]
}